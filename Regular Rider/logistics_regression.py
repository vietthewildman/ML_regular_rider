# -*- coding: utf-8 -*-
"""Regular_rider_Logistics_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1etiOA-KodbPjxsbYnX7PXypRiteGL1O8
"""

!pip install sweetviz

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # Linear algebra
import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt  # Matlab-style plotting
# Make sure plot shows immediately
# %matplotlib inline 

import seaborn as sns # Library for plotting
color = sns.color_palette()
sns.set_style('darkgrid')

from scipy import stats # Library for scientific computation
from scipy.stats import norm, skew # For some statistics

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.metrics import classification_report, roc_auc_score, plot_roc_curve

import sweetviz as sv

"""# 2. Predicting Bankruptcy using Logistic Regression

## 2.1. Data loading
The data consist 3 features & 1 label (0 - no regular rider & 1 - reguar rider)

The NA values in data set were marked as "?" which will be imputed by 0 later.

Data: https://raw.githubusercontent.com/pnhuy/datasets/master/vefa/uber_usage.csv
"""

# Data loading
DATA_LINK = 'https://raw.githubusercontent.com/pnhuy/datasets/master/vefa/uber_usage.csv'
df = pd.read_csv(DATA_LINK)
df = df.drop(columns=['CustomerID'])

df.describe()

sns.countplot(x="RegularRider", data=df)

# The data set contains no null value
df.isnull().sum()

"""## 2.2. Data preprocessing

* One-hot encodeing
* Standard scaling
* Train-test splitting

"""

num_cols = df.columns
print(num_cols)

train_df = pd.get_dummies(df, drop_first=True)
train_df.head()

num_cols = train_df.select_dtypes(include='number').columns
print(num_cols)
train_df[num_cols].head()

scaler = MinMaxScaler()
scaled_df = scaler.fit_transform(train_df[num_cols])

train_df[num_cols] = scaled_df
train_df.describe()

"""## 2.3. Train/Test Splitting"""

x = train_df.drop(columns=['RegularRider'])
y = train_df['RegularRider']

x_train, x_test, y_train, y_test = train_test_split(x, y ,test_size=0.4)

# Initiate the Logistic Regression model
logreg = LogisticRegression()

"""
Define the hyperparameter space
:params: penalty: Regularization method
:params: C: Regularization term, detail here: 
            https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
:params: solver: optimization algorithms
:params: class_weight: The “balanced” mode uses the values of y to automatically 
                       adjust weights inversely proportional to class frequencies 
                       in the input data as n_samples / (n_classes * np.bincount(y)).
"""

## TASK ##
"""
Replace None with the list of values for hyperparameter, 
please refer the scikit-learn for more information
"""

param_grid = {
    'penalty': ['l1', 'l2'], 
    'C': [-2, -1, -0.5, 0.5, 1],
    'class_weight': ['balanced']
}

## TASK ##

# Initiate the Gridsearch with Cross-validation
grid = GridSearchCV(logreg, param_grid, cv=5, scoring='roc_auc', verbose=True, n_jobs=-1)

# Fit the data and hyperparameter space to the model 
# and benchmark to find out the best classifier
# ~ 5 min in Colab
best_clf = grid.fit(x_train, y_train)

# Showing the best params set
best_clf.best_params_

"""## 2.4. Results"""

# Calculate the prediction in test set
y_pred = best_clf.predict(x_test)

# Print the metrics on test set
print(classification_report(y_test, y_pred))

# Print the AUC score
y_score = best_clf.decision_function(x_test) # best_clf.predict_proba
auc_score = roc_auc_score(y_test, y_score)
print(f"AUC = {auc_score}")

# Plot the ROC 
plot_roc_curve(best_clf, x_test, y_test)

"""## 2.5 SVM"""

from sklearn.svm import SVC

svc = SVC(class_weight='balanced', kernel='linear')

svc.fit(x_train, y_train)

y_pred = svc.predict(x_test)

print(classification_report(y_test, y_pred))

"""## 2.6. Hyperparameter optimization for SVM"""

param_grid = {
    'C': [0.01, 0.1, 1, 10, 50],
    'class_weight': ['balanced'],
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']
}

grid = GridSearchCV(svc, param_grid, cv=3, scoring='roc_auc', verbose=True, n_jobs=-1)

grid.fit(x_train, y_train)

grid.best_params_

best_model = grid.best_estimator_
y_pred = best_model.predict(x_test)
print(classification_report(y_test, y_pred))
