# -*- coding: utf-8 -*-
"""Real_estate_price_in_Houston

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jr7kd7LnayhG_DTQSIwWEiqObQCt2d_i
"""

!pip install sweetviz

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # Linear algebra
import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt  # Matlab-style plotting
# Make sure plot shows immediately
# %matplotlib inline 

import seaborn as sns # Library for plotting
color = sns.color_palette()
sns.set_style('darkgrid')

from scipy import stats # Library for scientific computation
from scipy.stats import norm, skew # For some statistics

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.metrics import classification_report, roc_auc_score, plot_roc_curve

import sweetviz as sv

"""# 1. Linear Regression"""

"""
Read data from CSV file and calculate some statistic.
The outcome/label/dependent variable is SalePrice.
The others are features/independent variable.
"""

TRAIN = 'https://github.com/SrikanthVelpuri/House-Prices-Advanced-Regression-Techniques/raw/master/train.csv'

## TASK ###
"""
* Read the csv in link `train` using pandas and assign to variable `df`
* Drop the columns `Id`
* Print the dimension of data
* Print some Desciptive statistics of data
"""

df = pd.read_csv(TRAIN)
df = df.drop(columns=['Id'])
print(df.shape)
df.describe()

## TASK ###

"""## 1.1. Exploratory data analysis"""

my_report = sv.analyze(df)
my_report.show_notebook() # Default arguments will generate to "SWEETVIZ_REPORT.html"

"""### The target
#### The distribution
"""

## TASK ##
"""
* Plot the distribution of outcome variable
"""

sns.distplot(df['SalePrice'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(df['SalePrice'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

## TASK ##

## TASK ##
"""
* Remove the data samples which have z-score >= 3
"""

df = df[(np.abs(stats.zscore(df.SalePrice)) < 3)]

## TASK ##

"""### Missing values
#### Calculating the rate of missing value
"""

## TASK ##
"""
* Remove the columns having missing rate > 10%
* For columns having missing rate <= 10%, fill the missing data with proper values:
    * Replace continous variable with means
    * Replace categorical variable with modes
"""

pass

## TASK ##

df_na = (df.isnull().sum() / len(df)) * 100
df_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'miss_rate' :df_na})

missing_data

missing_col_drop = list(missing_data[missing_data.miss_rate>10].index)

df_non_na = df.drop(columns=missing_col_drop)

# Calculate mean for every NUMERIC data field
means = df_non_na._get_numeric_data().mean()

# Fill NA by mean on NUMERIC data
df_non_na._get_numeric_data().fillna(means, inplace=True)

# Find the mode of NON-NUMERIC data
modes = df_non_na.mode()

# Fill NA by mode on NON-NUMERIC data field
df_non_na = df_non_na.fillna(modes.to_dict(orient='records')[0])

"""## 1.2. Remove highly correlated variables"""

## TASK ##
"""
In the dataset, we might want to remove the highly correlated variables.
In fact, for each pair of correlated variables, just remove one of them.

* Remove highly correlated variables (abs(r) > 0.8)

Hint: calculate correlation matrix might be slow with the whole dataset, 
using a fraction of dataset could be faster.
"""

## TASK ##

# Calculate the Correlation matrix of data frame without Dependent variable 
# (Just on 25% of data to speed up the calculation)
corr_matrix = df_non_na.drop(columns=['SalePrice'])._get_numeric_data().sample(frac=0.25).corr()

# Plot the heatmap of Correlation
f, ax = plt.subplots(figsize=(15, 15))
sns.heatmap(corr_matrix, vmax=1, annot=True, square=True);
plt.show()

# Create correlation matrix
corr_matrix = corr_matrix.abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.8
to_drop_corr = [column for column in upper.columns if any(upper[column] >= 0.8)]
print(to_drop_corr)

train_df = df_non_na.drop(columns=to_drop_corr)
train_df.head()

"""## 1.3. Normalize the numerical features"""

## TASK ##
"""
* Normal the numerical feature with MinMaxScaler
"""
## TASK ##

num_cols = train_df.drop(columns=['SalePrice']).select_dtypes(include='number').columns
print(num_cols)
train_df[num_cols].head()

from sklearn.preprocessing import MinMaxScaler, StandardScaler
scaler = MinMaxScaler()
scaled_df = scaler.fit_transform(train_df[num_cols])

train_df[num_cols] = scaled_df
train_df.describe()

"""## 1.4. Categorical variable handling"""

## TASK ##
"""
* Transform categorical variables into one-hot encoding variables
* Please remember remove one columns from the one-hot encoding columns,
it means that a n-category variabel would be transformed in to n-1 new columns
"""

## TASK ##

train_df = pd.get_dummies(train_df, drop_first=True)
train_df.head()

"""## 1.5. Feature selection

"""

## TASK ##
"""
* Extract ["OverallQual", "GrLivArea"]
from the full dataset for further training
"""
## TASK ##

ft = ["OverallQual", "GrLivArea", "SalePrice"]
train_df = train_df[ft]

"""## 1.6. Training/Test splitting"""

## TASK ##
"""
Split the dataframe into 2 part
* Training dataset (60%)
* Test dataset (40%)
"""
## TASK ##

X_train, X_test, y_train, y_test = train_test_split(train_df.drop(columns=['SalePrice']), train_df.SalePrice, test_size=0.4, random_state=42)

"""## 1.7. Linear regression -- scikit-learn"""

# Create linear regression object
regr = LinearRegression()
# Train the model using the training sets
regr.fit(X_train, y_train)

# Predict on Test set
y_pred = regr.predict(X_test)

# Evaluation on Test set
print(f'R-squared = {r2_score(y_test, y_pred):.4f}')
print(f'RMSE       = {mean_squared_error(y_test, y_pred)**0.5:.2f}')

print(regr.coef_)
print(regr.intercept_)
